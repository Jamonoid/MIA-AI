<p align="center">
  <h1 align="center">MIAâ€“AI
  </p>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.11%2B-blue?logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/license-AGPL--3.0-green" alt="License">
  <img src="https://img.shields.io/badge/VTube_Studio-OSC-purple" alt="VTube Studio">
  <img src="https://img.shields.io/badge/status-alpha-orange" alt="Status">
</p>

---

MIA es un pipeline de voz conversacional que convierte tu micrÃ³fono en un avatar interactivo:

```
ðŸŽ¤ MicrÃ³fono â†’ VAD â†’ STT â†’ RAG Memory â†’ LLM â†’ TTS â†’ ðŸ”Š Audio
                                                  â†“
                                            Lipsync â†’ VTube Studio / WebSocket
```

Todo funciona **localmente** y en **streaming** â€“ el avatar empieza a hablar antes de que el LLM termine de generar texto.

### Metas de latencia

| Etapa | Objetivo |
|---|---|
| Primer token LLM | < 300 ms |
| Primera salida de voz | < 900 ms |
| Lipsync update rate | 50â€“100 Hz |
| RAG retrieval | < 50 ms |

---

## Arquitectura

```
MIA-AI/
â”œâ”€â”€ config.yaml              # Toda la configuraciÃ³n (modelos, prompts, OSC, etc.)
â”œâ”€â”€ pyproject.toml            # Dependencias y metadata del proyecto
â”œâ”€â”€ AGENTS.md                 # GuÃ­a para contribuidores (IA o humanos)
â”‚
â”œâ”€â”€ src/mia/
â”‚   â”œâ”€â”€ main.py               # Punto de entrada (uv run mia)
â”‚   â”œâ”€â”€ config.py             # Carga tipada de YAML â†’ dataclasses
â”‚   â”œâ”€â”€ pipeline.py           # Orquestador async del pipeline completo
â”‚   â”‚
â”‚   â”œâ”€â”€ audio_io.py           # Captura de mic + cola de reproducciÃ³n
â”‚   â”œâ”€â”€ vad.py                # Voice Activity Detection (RMS)
â”‚   â”œâ”€â”€ stt_whispercpp.py     # Speech-to-Text (faster-whisper)
â”‚   â”œâ”€â”€ llm_llamacpp.py       # LLM local (llama-cpp-python)
â”‚   â”œâ”€â”€ llm_lmstudio.py       # LLM vÃ­a LM Studio (API OpenAI)
â”‚   â”œâ”€â”€ tts_xtts.py           # Text-to-Speech con chunking (XTTS v2)
â”‚   â”œâ”€â”€ rag_memory.py         # Memoria conversacional (ChromaDB)
â”‚   â”‚
â”‚   â”œâ”€â”€ lipsync.py            # SincronizaciÃ³n labial (RMS â†’ mouth_open)
â”‚   â”œâ”€â”€ vtube_osc.py          # Control de VTube Studio vÃ­a OSC/UDP
â”‚   â””â”€â”€ ws_server.py          # WebSocket server para frontend propio
â”‚
â”œâ”€â”€ tests/                    # Tests unitarios
â”œâ”€â”€ models/                   # Modelos GGUF (no incluidos)
â”œâ”€â”€ voices/                   # Samples de voz para clonaciÃ³n (no incluidos)
â””â”€â”€ data/chroma_db/           # Vector store persistente (auto-generado)
```

---

## InstalaciÃ³n

### Requisitos previos

- **Python 3.11+**
- **[uv](https://docs.astral.sh/uv/)** (gestor de paquetes rÃ¡pido)
- **GPU con CUDA** (recomendado para STT/LLM/TTS)

### 1. Clonar y crear entorno

```bash
git clone https://github.com/Jamonoid/MIA-AI.git
cd MIA-AI
uv venv
uv pip install -e ".[dev]"
```

### 2. Instalar dependencias ML

Estas dependencias son pesadas y requieren compilaciÃ³n C++/CUDA:

```bash
# STT (faster-whisper descarga el modelo automÃ¡ticamente)
pip install faster-whisper

# LLM
pip install llama-cpp-python

# TTS (requiere Visual Studio Build Tools en Windows)
pip install TTS
```

> ** Nota Windows:** Si `TTS` falla al compilar, instala [Visual Studio Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/) con el workload "C++ build tools".

### 3. Descargar modelos

Coloca los modelos en las rutas definidas en `config.yaml`:

```bash
mkdir models voices
```

| Modelo | Recomendado | DÃ³nde |
|---|---|---|
| **LLM** | [Llama 3 8B Q4_K_M](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF) | `./models/llama-3-8b.gguf` |
| **STT** | Whisper `base` (auto-descarga) | AutomÃ¡tico |
| **TTS Voice** | WAV de referencia (~10s) | `./voices/female_01.wav` |

---

## âš™ï¸ ConfiguraciÃ³n

Todo se controla desde `config.yaml`:

```yaml
prompt:
  system: "Eres MIA, una asistente virtual inteligente y amigable."

models:
  llm:
    backend: "llamacpp"       # "llamacpp" | "lmstudio"
    path: "./models/llama-3-8b.gguf"
    context_size: 2048
    n_gpu_layers: -1          # -1 = todas las capas en GPU
    # LM Studio (solo si backend: "lmstudio")
    base_url: "http://localhost:1234/v1"
    model_name: "default"
  stt:
    model_size: "base"        # tiny | base | small | medium
    language: "es"
  tts:
    voice_path: "./voices/female_01.wav"
    chunk_size: 150            # caracteres por chunk TTS

rag:
  enabled: true
  top_k: 3
  max_docs: 5000

osc:
  ip: "127.0.0.1"
  port: 9000                  # Puerto de VTube Studio
  mapping:
    mouth_open: "MouthOpen"
    blink: "EyeBlink"

websocket:
  host: "127.0.0.1"
  port: 8765
  enabled: true
```

> Consulta [config.yaml](config.yaml) para ver todas las opciones disponibles.

---

## â–¶ï¸ Uso

### Ejecutar MIA

```bash
uv run mia
```

MIA se iniciarÃ¡, cargarÃ¡ los modelos y comenzarÃ¡ a escuchar por el micrÃ³fono.

### Conectar con VTube Studio

1. Abre **VTube Studio**
2. Ve a **Settings â†’ VTube Studio API â†’ OSC Receiver**
3. Habilita OSC y configura el puerto `9000`
4. Los parÃ¡metros `MouthOpen` y `EyeBlink` se actualizarÃ¡n automÃ¡ticamente

### Conectar un frontend propio

MIA expone un servidor WebSocket en `ws://127.0.0.1:8765` que envÃ­a mensajes JSON:

```json
{"type": "mouth", "value": 0.42}
{"type": "emotion", "value": "happy"}
{"type": "subtitle", "role": "assistant", "text": "Â¡Hola!"}
{"type": "status", "value": "listening"}
```

### Usar LM Studio como backend de LLM

[LM Studio](https://lmstudio.ai/) es la forma mÃ¡s fÃ¡cil de correr modelos locales â€” no requiere compilar `llama-cpp-python`.

1. **Descarga e instala** [LM Studio](https://lmstudio.ai/)
2. **Carga un modelo** desde la UI (ej. Llama 3 8B)
3. **Inicia el servidor local** â†’ "Local Server" â†’ Start
4. **Cambia el backend** en `config.yaml`:

```yaml
models:
  llm:
    backend: "lmstudio"
    base_url: "http://localhost:1234/v1"
```

5. **Ejecuta MIA:** `uv run mia`

> **ðŸ’¡ Ventajas:** No necesita compilaciÃ³n C++. Se puede cambiar de modelo desde la UI de LM Studio sin reiniciar MIA. GPU nativa.

---

## ðŸ§ª Tests

```bash
uv run pytest -v
```

Los tests cubren:
- Carga y validaciÃ³n de config YAML
- ConstrucciÃ³n de prompts (con RAG y sin RAG)
- Chunking de texto para TTS
- VAD (detecciÃ³n de silencio/habla)
- Lipsync (mapeo RMS â†’ mouth_open)

---

## ðŸ§  Memoria RAG

MIA recuerda conversaciones pasadas gracias a un sistema RAG local:

- **Almacenamiento:** ChromaDB (persistente en `./data/chroma_db/`)
- **Embeddings:** `all-MiniLM-L6-v2` (~80 MB)
- **Funcionamiento:** Al final de cada turno, se almacena el par `(usuario, MIA)`. En cada nueva pregunta, se recuperan los fragmentos mÃ¡s relevantes y se inyectan en el prompt del LLM.
- **Desactivar:** Pon `rag.enabled: false` en `config.yaml`

---

## ðŸ“Š Stack tecnolÃ³gico

| Componente | TecnologÃ­a |
|---|---|
| STT | [faster-whisper](https://github.com/SYSTRAN/faster-whisper) (CTranslate2) |
| LLM | [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) o [LM Studio](https://lmstudio.ai/) |
| TTS | [Coqui TTS](https://github.com/coqui-ai/TTS) (XTTS v2) |
| VAD | Energy-based (RMS, zero deps) |
| Lipsync | RMS con smoothing exponencial |
| Avatar | VTube Studio vÃ­a OSC / WebSocket |
| Memoria | ChromaDB + sentence-transformers |
| Audio | sounddevice (PortAudio) |
| Config | YAML â†’ dataclasses tipadas |

---

